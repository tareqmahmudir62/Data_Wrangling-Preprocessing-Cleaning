##  <b> Project Overview </b>  ##

Real-world data rarely comes clean. Using Python and its libraries, I have gathered data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. I have documented my wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and very simple visualizations using Python (and its libraries).

The dataset that I will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because "they're good dogs Brent." WeRateDogs has over 4 million followers and has received international media coverage.

![image](https://user-images.githubusercontent.com/97672246/195761406-2cb9340c-75e6-480e-81c7-81fa668ff45d.png)
Image via Boston Magazine

##  <b> Project Steps Overview </b>  ##

tep 1: Gathering data

Step 2: Assessing data

Step 3: Cleaning data

Step 4: Storing data

Step 5: Analyzing, and visualizing data

Step 6: Reporting
    a) data whangling effort
    b) data analyses and visualizations
    
##  <b> Softwares/tools used </b>  ##

      Jupyter Notebook
      pandas
      NumPy
      requests
      tweepy
      json

##  <b> Files Included </b>  ##
     1.wrangle_act.ipynb: code for gathering, assessing, cleaning, analyzing, and visualizing data
     2.wrangle_report.pdf: documentation for data wrangling steps: gather, assess, and clean
     3.act_report.pdf: documentation of analysis and insights into final data
     4.twitter_archive_enhanced.csv: file as given
     5.image_predictions.tsv: file downloaded programmatically
     6.tweet_json.txt: file constructed via API
     7.twitter_archive_master.csv: combined and cleaned data
